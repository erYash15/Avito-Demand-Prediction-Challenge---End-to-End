{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict demand for an online classified ad – Avito Demand Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains Final Pipleine for preprocessing and perfomance metrics in individual functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing Libraries\n",
    "2. Loading Files\n",
    "3. Feature Engineering Pipeline\n",
    "4. Loading Model\n",
    "4. final_fun_1 \n",
    "5. final_fun_2 \n",
    "6. Driver Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import cv2\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import (ImageDataGenerator,\n",
    "                                                  img_to_array, load_img)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, Flatten, LSTM, concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_desc = 300000\n",
    "max_length_desc = 200\n",
    "\n",
    "max_features_title = 150000\n",
    "max_length_title = 8\n",
    "\n",
    "embedding_vector_size = 300\n",
    "\n",
    "region_length = 29\n",
    "city_length = 1644\n",
    "parent_category_name_length = 10\n",
    "category_name_length = 48\n",
    "param_1_length = 360\n",
    "param_2_length = 233\n",
    "param_3_length = 965\n",
    "user_type_length = 4\n",
    "image_top_1_C_length = 3039\n",
    "region__city_length = 1702\n",
    "param_1__param_2__param_3_length = 1920\n",
    "parent_category_name__user_type_length = 28\n",
    "category_name__user_type_length = 141\n",
    "region__user_type_length = 85\n",
    "\n",
    "num_features = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./bin/embedding_matrix_title.pkl\", \"rb\") as input_file:\n",
    "    embedding_matrix_title = pickle.load(input_file)\n",
    "with open(\"./bin/embedding_matrix_desc.pkl\", \"rb\") as input_file:\n",
    "    embedding_matrix_desc = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FE_pipeline(df):\n",
    "\n",
    "    ## Creating duplicate features\n",
    "    df['image_top_1_C'] = df['image_top_1']\n",
    "    df['image_top_1_N'] = df['image_top_1']\n",
    "\n",
    "    print(\"Duplicated columns created.\")\n",
    "    \n",
    "    ## Filling missing value\n",
    "    print(\"Filling missing value.\")\n",
    "    # Categorical and Text Feature\n",
    "    df['param_1'] = df['param_1'].fillna('null')\n",
    "    df['param_2'] = df['param_2'].fillna('null')\n",
    "    df['param_3'] = df['param_3'].fillna('null')\n",
    "    df['description'] = df['description'].fillna('null')\n",
    "    df['image_top_1_C'] = df['image_top_1_C'].astype(str).fillna('null')\n",
    "    \n",
    "    print(\"Missing value filled for categorical and text feature.\")\n",
    "    \n",
    "    # Numerical Feature\n",
    "    price_average_df = pd.read_csv('./bin/price_category_name_mean.csv')\n",
    "    image_top_1_average_df = pd.read_csv('./bin/image_top_1_N_category_name_mean.csv')\n",
    "    \n",
    "    def fill_missing_price(df, price_average_df):\n",
    "        df = df.merge(price_average_df, on='category_name', how='left').reset_index(drop=True)\n",
    "        df.loc[df[df['price_x'].isnull() == True].index, 'price_x'] = df[df['price_x'].isnull() == True]['price_y']\n",
    "        df = df.drop(columns=['price_y'])\n",
    "        df.rename(columns = {'price_x':'price'}, inplace = True)\n",
    "        return df\n",
    "    \n",
    "    def fill_missing_image_top_1(df, image_top_1_average_df):\n",
    "        df = df.merge(image_top_1_average_df, on='category_name', how='left').reset_index(drop=True)\n",
    "        df.loc[df[df['image_top_1_N_x'].isnull() == True].index, 'image_top_1_N_x'] = df[df['image_top_1_N_x'].isnull() == True]['image_top_1_N_y']\n",
    "        df = df.drop(columns=['image_top_1_N_y'])\n",
    "        df.rename(columns = {'image_top_1_N_x':'image_top_1_N'}, inplace = True)\n",
    "        return df\n",
    "\n",
    "    df = fill_missing_price(df, price_average_df)\n",
    "    df = fill_missing_image_top_1(df, image_top_1_average_df)\n",
    "\n",
    "    print(\"Missing value filled for numerical feature.\")\n",
    "    \n",
    "    # Image Feature\n",
    "    train_df_image_path = os.path.join('avito-demand-prediction','train_jpg')\n",
    "    \n",
    "    def fill_missing_imagepath(x, main_path):\n",
    "        # missing images are filled with default_image.jpg\n",
    "        if x!=x:  \n",
    "            return str(os.path.join(main_path, 'default_image.jpg'))\n",
    "        return os.path.join(main_path, 'avito-demand-prediction', str(x)+'.jpg')\n",
    "    \n",
    "    df['image_fullpath'] = df['image'].apply(fill_missing_imagepath, main_path = train_df_image_path)\n",
    "    \n",
    "    print(\"Missing value filled for Image feature.\")\n",
    "    \n",
    "    ## Interactive Features \n",
    "    df['region__city'] = df['region'] + df['city']\n",
    "    df['param_1__param_2__param_3'] = df['param_1'] + df['param_2'] + df['param_3']\n",
    "    df['parent_category_name__user_type'] = df['parent_category_name'] + df['user_type']\n",
    "    df['category_name__user_type'] = df['category_name'] + df['user_type']\n",
    "    df['region__user_type'] = df['region'] + df['user_type']\n",
    "\n",
    "    print(\"Interactive features created.\")\n",
    "    \n",
    "    ## More Features\n",
    "    # calling the text_featuring function which extract features from the title and description.\n",
    "    df[\"title_words_length\"] = df[\"title\"].apply(lambda x: len(x.split()))\n",
    "    df[\"description_words_length\"] = df[\"description\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "    df['symbol1_count'] = df['description'].str.count('↓')\n",
    "    df['symbol2_count'] = df['description'].str.count('\\*')\n",
    "    df['symbol3_count'] = df['description'].str.count('✔')\n",
    "    df['symbol4_count'] = df['description'].str.count('❀')\n",
    "    df['symbol5_count'] = df['description'].str.count('➚')\n",
    "    df['symbol6_count'] = df['description'].str.count('ஜ')\n",
    "    df['symbol7_count'] = df['description'].str.count('.')\n",
    "    df['symbol8_count'] = df['description'].str.count('!')\n",
    "    df['symbol9_count'] = df['description'].str.count('\\?')\n",
    "    df['symbol10_count'] = df['description'].str.count('  ')\n",
    "    df['symbol11_count'] = df['description'].str.count('-')\n",
    "    df['symbol12_count'] = df['description'].str.count(',')\n",
    "    \n",
    "    print(\"More features created.\")\n",
    "    \n",
    "    ## Feature Engineering\n",
    "    # Categorical Features\n",
    "    \n",
    "    def categorical_encoder_P(Series, name): # Pipeline\n",
    "        '''This function encode the categorical feature which we will use in NN along with embedding layer'''\n",
    "        tokeniser = pickle.load(open('./bin/'+name+'_tokeniser.pkl', 'rb'))\n",
    "        Series = np.array(tokeniser.texts_to_sequences(Series)).astype(np.int32)\n",
    "        Series = Series[:,0]\n",
    "        return Series\n",
    "    \n",
    "    df['region'] = categorical_encoder_P(df['region'], 'region') #1\n",
    "    df['city'] = categorical_encoder_P(df['city'], 'city') #2\n",
    "    df['parent_category_name'] = categorical_encoder_P(df['parent_category_name'], 'parent_category_name') #3\n",
    "    df['category_name'] = categorical_encoder_P(df['category_name'], 'category_name') #4\n",
    "    df['param_1'] = categorical_encoder_P(df['param_1'], 'param_1') #5\n",
    "    df['param_2'] = categorical_encoder_P(df['param_2'], 'param_2') #6\n",
    "    df['param_3'] = categorical_encoder_P(df['param_3'], 'param_3') #7\n",
    "    df['user_type'] = categorical_encoder_P(df['user_type'], 'user_type') #8\n",
    "\n",
    "    df['region__city'] = categorical_encoder_P(df['region__city'], 'region__city') #9\n",
    "    df['param_1__param_2__param_3'] = categorical_encoder_P(df['param_1__param_2__param_3'], 'param_1__param_2__param_3') #10\n",
    "    df['parent_category_name__user_type'] = categorical_encoder_P(df['parent_category_name__user_type'], 'parent_category_name__user_type') #11\n",
    "    df['category_name__user_type'] = categorical_encoder_P(df['category_name__user_type'], 'category_name__user_type') #12\n",
    "    df['region__user_type'] = categorical_encoder_P(df['region__user_type'], 'region__user_type') #13\n",
    "    df['image_top_1_C'] = categorical_encoder_P(df['image_top_1_C'], 'image_top_1_C') #14\n",
    "    \n",
    "    \n",
    "    print(\"Encoding Categorical Features.\")\n",
    "    \n",
    "    # Numerical Feature\n",
    "    \n",
    "    columns = ['price', 'item_seq_number', 'image_top_1_N', 'title_words_length', \n",
    "             'description_words_length',\n",
    "             'symbol1_count', 'symbol2_count', 'symbol3_count', 'symbol4_count',\n",
    "             'symbol5_count', 'symbol6_count', 'symbol7_count', 'symbol8_count',\n",
    "             'symbol9_count', 'symbol10_count', 'symbol11_count', 'symbol12_count']\n",
    "\n",
    "    # In this subsection we have will transform the numerical features. \n",
    "    # Applying logirthmic transformation to avoid bais caused by normalization. Then batch_normalization layer.\n",
    "\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: np.log10(x+1)) # adding 1 as bias\n",
    "\n",
    "    print(\"Numerical Features transformed\")\n",
    "\n",
    "    # Text Feature Engineering\n",
    "    \n",
    "    def text_clean(text):\n",
    "        '''This function clean the russian text'''\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        clean = re.sub(r\"[,.;@#?!&$-]+\\ *\", \" \", text)\n",
    "        return clean\n",
    "\n",
    "    df['title'] = df['title'].apply(text_clean)\n",
    "    df['description'] = df['description'].apply(text_clean)\n",
    "    \n",
    "    print(\"Text title and description are cleaned.\")\n",
    "    \n",
    "    with open(\"./bin/tokenizer_title.pkl\", \"rb\") as input_file:\n",
    "        tokenizer_title = pickle.load(input_file)\n",
    "    with open(\"./bin/tokenizer_desc.pkl\", \"rb\") as input_file:\n",
    "        tokenizer_desc = pickle.load(input_file)\n",
    "\n",
    "    def encoder(train, tokenizer, max_length):\n",
    "        ''' This function perform the tokenization and then convert words to integers and then perform padding and returns the values '''\n",
    "        encoded_str = tokenizer.texts_to_sequences(train)\n",
    "        padded_str = np.array(pad_sequences(encoded_str, maxlen=max_length, padding='post')).astype(np.float64)\n",
    "        return padded_str\n",
    "\n",
    "    padded_title_df = encoder(df['title'], tokenizer_title, 8)\n",
    "    padded_desc_df = encoder(df['description'], tokenizer_desc, 200)\n",
    "\n",
    "        \n",
    "    return df, padded_title_df, padded_desc_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Preprocessing(df):\n",
    "    \n",
    "    df = df.drop(columns=['item_id', 'user_id', 'title', 'description', \n",
    "                          'activation_date', 'image', 'image_top_1', 'image_fullpath'])\n",
    "\n",
    "    df_cat = df[['region', 'city', 'parent_category_name', 'category_name',\n",
    "                 'param_1', 'param_2', 'param_3', 'user_type', 'image_top_1_C',\n",
    "                 'region__city', 'param_1__param_2__param_3',\n",
    "                 'parent_category_name__user_type', 'category_name__user_type',\n",
    "                 'region__user_type']]\n",
    "    \n",
    "    df_num = df[['price', 'item_seq_number', 'image_top_1_N', 'title_words_length', \n",
    "                 'description_words_length',\n",
    "                 'symbol1_count', 'symbol2_count', 'symbol3_count', 'symbol4_count',\n",
    "                 'symbol5_count', 'symbol6_count', 'symbol7_count', 'symbol8_count',\n",
    "                 'symbol9_count', 'symbol10_count', 'symbol11_count', 'symbol12_count']]\n",
    "\n",
    "    df = pd.concat([df_cat, df_num],axis=1)\n",
    "    \n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "def rmse_score():\n",
    "    return make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "\n",
    "    input1 = Input(shape= (max_length_title,), name='title')\n",
    "    x1 = Embedding(\n",
    "        max_features_title,\n",
    "        embedding_vector_size,\n",
    "        weights = [embedding_matrix_title],\n",
    "        trainable=False)(input1)\n",
    "    x1 = LSTM(64, return_sequences=False)(x1)\n",
    "    x1 = Flatten()(x1)\n",
    "\n",
    "    input2 = Input(shape= (max_length_desc,), name='description')\n",
    "    x2 = Embedding(\n",
    "        max_features_desc,\n",
    "        embedding_vector_size,\n",
    "        weights = [embedding_matrix_desc],\n",
    "        trainable=False)(input2)\n",
    "    x2 = LSTM(64, return_sequences=False)(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "\n",
    "    input3 = Input(shape= (num_features, ), name='numerical')\n",
    "    x3 = BatchNormalization()(input3)\n",
    "    x3 = Dense(64, activation='relu')(x3)\n",
    "    x3 = Dense(16, activation='relu')(x3)\n",
    "\n",
    "\n",
    "    inputc1 = Input(shape= (1,), name='region')\n",
    "    c1 = Embedding(input_dim=region_length, output_dim=2, trainable=True)(inputc1)\n",
    "    c1 = Flatten()(c1)\n",
    "\n",
    "    inputc2 = Input(shape= (1,), name='city')\n",
    "    c2 = Embedding(input_dim=city_length, output_dim=2, trainable=True)(inputc2)\n",
    "    c2 = Flatten()(c2)\n",
    "\n",
    "    inputc3 = Input(shape= (1,), name='parent_category_name')\n",
    "    c3 = Embedding(input_dim=parent_category_name_length, output_dim=2, trainable=True)(inputc3)\n",
    "    c3 = Flatten()(c3)\n",
    "\n",
    "    inputc4 = Input(shape= (1,), name='category_name')\n",
    "    c4 = Embedding(input_dim=category_name_length, output_dim=2, trainable=True)(inputc4)\n",
    "    c4 = Flatten()(c4)\n",
    "\n",
    "    inputc5 = Input(shape= (1,), name='param_1')\n",
    "    c5 = Embedding(input_dim=param_1_length, output_dim=2, trainable=True)(inputc5)\n",
    "    c5 = Flatten()(c5)\n",
    "\n",
    "    inputc6 = Input(shape= (1,), name='param_2')\n",
    "    c6 = Embedding(input_dim=param_2_length, output_dim=2, trainable=True)(inputc6)\n",
    "    c6 = Flatten()(c6)\n",
    "\n",
    "    inputc7 = Input(shape= (1,), name='param_3')\n",
    "    c7 = Embedding(input_dim=param_3_length, output_dim=2, trainable=True)(inputc7)\n",
    "    c7 = Flatten()(c7)\n",
    "\n",
    "    inputc8 = Input(shape= (1,), name='user_type')\n",
    "    c8 = Embedding(input_dim=user_type_length, output_dim=2, trainable=True)(inputc8)\n",
    "    c8 = Flatten()(c8)\n",
    "\n",
    "    inputc9 = Input(shape= (1,), name='image_top_1_C')\n",
    "    c9 = Embedding(input_dim=image_top_1_C_length, output_dim=2, trainable=True)(inputc9)\n",
    "    c9 = Flatten()(c9)\n",
    "\n",
    "    inputc10 = Input(shape= (1,), name='region__city')\n",
    "    c10 = Embedding(input_dim=region__city_length, output_dim=2, trainable=True)(inputc10)\n",
    "    c10 = Flatten()(c10)\n",
    "\n",
    "    inputc11 = Input(shape= (1,), name='param_1__param_2__param_3')\n",
    "    c11 = Embedding(input_dim=param_1__param_2__param_3_length, output_dim=2, trainable=True)(inputc11)\n",
    "    c11 = Flatten()(c11)\n",
    "\n",
    "    inputc12 = Input(shape= (1,), name='parent_category_name__user_type')\n",
    "    c12 = Embedding(input_dim=parent_category_name__user_type_length, output_dim=2, trainable=True)(inputc12)\n",
    "    c12 = Flatten()(c12)\n",
    "\n",
    "    inputc13 = Input(shape= (1,), name='category_name__user_type')\n",
    "    c13 = Embedding(input_dim=category_name__user_type_length, output_dim=2, trainable=True)(inputc13)\n",
    "    c13 = Flatten()(c13)\n",
    "\n",
    "    inputc14 = Input(shape= (1,), name='region__user_type')\n",
    "    c14 = Embedding(input_dim=region__user_type_length, output_dim=2, trainable=True)(inputc14)\n",
    "    c14 = Flatten()(c14)\n",
    "\n",
    "    output = concatenate([x1, x2, x3, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14])\n",
    "    output = Dropout(0.3)(output)\n",
    "    output = Dense(128, activation='relu')(output)\n",
    "    output = Dropout(0.3)(output)\n",
    "    output = Dense(64, activation='relu')(output)\n",
    "    output = Dropout(0.3)(output)\n",
    "    output = Dense(32, activation='relu')(output)\n",
    "    output = Dense(1, activation='softmax')(output)\n",
    "\n",
    "    model = Model(inputs = [input1, input2, input3, inputc1, inputc2, inputc3, inputc4, inputc5, inputc6, inputc7, inputc8, inputc9, inputc10, inputc11, inputc12, inputc13, inputc14], outputs = output)\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01,clipnorm=1.0,clipvalue=0.05), \n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    try:\n",
    "        model.load_weights('./models/best_model_4.h5')\n",
    "        print(\"Model Loaded!\")\n",
    "    except:\n",
    "        print(\"No Model Available\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded!\n"
     ]
    }
   ],
   "source": [
    "model = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. final_fun_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_1(X):\n",
    "\n",
    "    print(\"Raw_Data:\", X.shape)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    X_test, padded_title_test, padded_desc_test  = FE_pipeline(X)\n",
    "    X_test = Data_Preprocessing(X_test)\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(\"Summary of all Fetaures\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"1. X_test:\", X_test.shape)\n",
    "    print(\"2. test Title Encoded\", padded_title_test.shape)\n",
    "    print(\"3. test Description Encoded\", padded_desc_test.shape)\n",
    "    \n",
    "    region_test = X_test[:,0]\n",
    "    city_test = X_test[:,1]\n",
    "    parent_category_name_test = X_test[:,2]\n",
    "    category_name_test = X_test[:,3]\n",
    "    param_1_test = X_test[:,4]\n",
    "    param_2_test = X_test[:,5]\n",
    "    param_3_test = X_test[:,6]\n",
    "    user_type_test = X_test[:,7]\n",
    "    image_top_1_C_test = X_test[:,8]\n",
    "    region__city_test = X_test[:,9]\n",
    "    param_1__param_2__param_3_test = X_test[:,10]\n",
    "    parent_category_name__user_type_test = X_test[:,11]\n",
    "    category_name__user_type_test = X_test[:,12]\n",
    "    region__user_type_test = X_test[:,13]\n",
    "\n",
    "    ntest = X_test[:,14:]\n",
    "\n",
    "    # predicting on batch caused memory error prediction value by value\n",
    "    y_pred = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        ypred = model.predict([padded_title_test[i,:], padded_desc_test[i,:], ntest[i,:], \n",
    "                        region_test[i,:], city_test[i,:], parent_category_name_test[i,:], \n",
    "                        category_name_test[i,:], param_1_test[i,:], param_2_test[i,:], param_3_test[i,:], \n",
    "                        user_type_test[i,:], image_top_1_C_test[i,:], region__city_test[i,:], \n",
    "                        param_1__param_2__param_3_test[i,:], parent_category_name__user_type_test[i,:], \n",
    "                        category_name__user_type_test[i,:], region__user_type_test[i,:]])\n",
    "        y_pred.append(ypred)\n",
    "    print(\"Predictions Done!\")\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. final_fun_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_2(X, Y):\n",
    "    y_pred  = final_fun_1(X)\n",
    "    return rmse(y_pred,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Driver Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw_Data: (100, 17)\n",
      "==================================================\n",
      "Duplicated columns created.\n",
      "Filling missing value.\n",
      "Missing value filled for categorical and text feature.\n",
      "Missing value filled for numerical feature.\n",
      "Missing value filled for Image feature.\n",
      "Interactive features created.\n",
      "More features created.\n",
      "Encoding Categorical Features.\n",
      "Numerical Features transformed\n",
      "Text title and description are cleaned.\n",
      "==================================================\n",
      "Summary of all Fetaures\n",
      "==================================================\n",
      "1. X_test: (100, 31)\n",
      "2. test Title Encoded (100, 8)\n",
      "3. test Description Encoded (100, 200)\n",
      "Predictions Done!\n",
      "==================================================\n",
      "RMSE Score :  0.244549768034265\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    n = 100 # sample of 100 points\n",
    "    train_df = pd.read_csv('./avito-demand-prediction/train.csv', parse_dates=[\"activation_date\"])\n",
    "\n",
    "    train_df = train_df.sample(n)\n",
    "    X = train_df.drop(columns=['deal_probability'])\n",
    "    y = train_df['deal_probability']\n",
    "\n",
    "    rmse_ = final_fun_2(X, y)\n",
    "    print(\"=\"*50)\n",
    "    print(\"RMSE Score : \", rmse_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3bfafb6b67a46a0be8a83346ceab811c6fa8466abc10bfdf93cbd60f80d11249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
